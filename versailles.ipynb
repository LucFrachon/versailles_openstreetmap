{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project: OpenStreetMap\n",
    "## Discovering Versailles (France) and its surroundings\n",
    "![chateau](img/799px-Versailles-FacadeJardin.jpg)\n",
    "---\n",
    "## Introduction and Map Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Garden façace of the Château de Versailles. The main building is 415m wide (1,362ft) and the garden covers over 1,500 acres (it was 12,300 acres before the French Revolution). The 'Grand Canal' (main piece of water) alone is a mile long and took 13 years to dig.",
    "collapsed": false,
    "label": "fig:chateau",
    "widefigure": true
   },
   "source": [
    "![map](img/map.png)\n",
    "\n",
    "The map used in this project is from OpenStreetMap, obtained as a custom extract from mapzen.com ([link to dataset](https://s3.amazonaws.com/mapzen.odes/ex_DRcFT2edje1bEvDwVEKv4jZvAAvP8.osm.bz2)).\n",
    "\n",
    "I lived in this area (in Saint-Germain-En-Laye, precisely) for 7 years before moving to Romania, so I was interested in analysing and possibly improving the data.  \n",
    "Moreover, Versailles is famous around the world for its stunning and gigantic palace, built in its current configuration by Louis XIV from 1662. I thought it would be an area that the reviewers could relate to and might be interested in discovering in more detail.\n",
    "\n",
    "**Note**: By default, any code is hidden in the HTML version of this document. However there is a \"Show Code\" button at the top which allows the reader to see the code that was used to generate any result shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the project, we want to assess and improve the quality of the data.\n",
    "The full data is 225MB uncompressed. To speed up development time, the code was first tested using a small sample of the data (every 25th top level element), which represents about 9MB.\n",
    "\n",
    "**Note on efficiency**: The code snippets below have been written to be largely independent from one another and they execute distinct tasks in line with the document structure. It would have been far more efficient from a computational point of view to execute all checks within one iterative pass of the full OSM XML document, rather than separate tasks and execute each in its own pass. However in the context of this project, we felt it was more instructional to have this structured approach because the focus of the project is more on the data wrangling methodology than on the subsequent use of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1. Validity / Uniformity\n",
    "\n",
    "Here we want to ensure that the data provided conforms to certain rules and schemas and that values are consistent. More specifically, we will assess the following points:\n",
    "\n",
    "* Ensure that all elements in the XML file have a limited and consistent set of tags\n",
    "* Check that latitude and longitude are expressed in a consistent and usable format\n",
    "* Make sure postcodes respect the French post format\n",
    "* Ensure that street types are logical and consistently spelled throughout the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.a. Data tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect and count the unique element tags in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 24412,\n",
      " 'nd': 1332859,\n",
      " 'node': 1007548,\n",
      " 'osm': 1,\n",
      " 'relation': 1942,\n",
      " 'tag': 483337,\n",
      " 'way': 159884}\n"
     ]
    }
   ],
   "source": [
    "filename = './Versailles.osm/Versailles.osm'\n",
    "\n",
    "\n",
    "def count_tags(filename):\n",
    "    ''' Identifies unique tag types and counts occurences for each\n",
    "    In:\n",
    "        filename (string): Path to OSM XML file to assess\n",
    "    Out:\n",
    "        dict: Unique tag types as keys, counts as values   \n",
    "    '''   \n",
    "    tags = {}\n",
    "    context = ET.iterparse(filename, events = ('start',))  # Detect starts only\n",
    "    for event, elem in context:     # Go through each element in the OSM XLM file\n",
    "        if not(elem.tag in tags):   # Create new dict key if not already there   \n",
    "            tags[elem.tag] = 0\n",
    "        tags[elem.tag] += 1\n",
    "    return tags\n",
    "\n",
    "pprint.pprint(count_tags(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation for OSM XML files, these tags appear correct. We also note that the data contains over one million nodes, around 160,000 ways and 480,000 tags, giving us a nice dataset to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.b. Latitude / longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data consistent and usable, it is desirable to have latitde and longitude expressed as floating point numbers (as opposed to strings such as \"48.92N\" / \"2.13E\"). At the same time, we will want to assess that all the element positions are within the bounds of the map extract that we selected (ie. [48.758 - 48.919] / [2.019 - 2.178])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Correct': 1007548,\n",
      " 'Empty': 0,\n",
      " 'Non_number': 0,\n",
      " 'Null': 0,\n",
      " 'Out_of_bounds': 0}\n"
     ]
    }
   ],
   "source": [
    "lat_bounds = (48.7582257, 48.9188896)\n",
    "lon_bounds = (2.0190811, 2.1783828)\n",
    "\n",
    "def is_number(s):\n",
    "    ''' Returns True if string or float s is or can be coerced into a float, \n",
    "    False otherwise'''\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def is_within_bounds(f, bounds):\n",
    "    ''' Checks whether f is within the interval defined by bounds\n",
    "    In:\n",
    "        f (float): Value to test\n",
    "        bounds (tuple): lower, upper bounds of the interval where f might be \n",
    "                        included.\n",
    "    Out:\n",
    "        bool: True if f is contained in the interval (equality allowed),\n",
    "              False if f is strictly lower than lower bound or strictly greater\n",
    "              than upper bound.\n",
    "    '''\n",
    "    if f >= bounds[0] and f <= bounds[1]:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "def check_positions(filename):  \n",
    "    ''' Checks latitude and longitude for validity and accuracy\n",
    "    In:\n",
    "        filename (string): Path to the file to check\n",
    "    Out:\n",
    "        dict: Values = Counts of the following occurences (used as keys):\n",
    "            Null -- at least one of latitude and longitude is None\n",
    "            Empty -- at least one of latitude and longitude is \"\"\n",
    "            Non_number -- at least one of latitude and longitude is not a \n",
    "                          float or cannot be coerced into a float\n",
    "            Out_of_bounds -- at least one of latitude and longitude is outside\n",
    "                             the bounds of the selected map area\n",
    "            Correct -- none of the above occurs\n",
    "    '''\n",
    "    counts = {'Null': 0, 'Empty': 0, 'Non_number': 0, 'Out_of_bounds': 0, \n",
    "              'Correct': 0}\n",
    "    context = ET.iterparse(filename, events = ('start', )) # Detect starts only\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'node':  # Check for each node element\n",
    "            lat = elem.get('lat').strip()  # Get rid of unwanted spaces\n",
    "            lon = elem.get('lon').strip()\n",
    "            if (lat is None or lon is None):\n",
    "                counts['Null'] += 1\n",
    "            elif (lat == \"\" or lon == \"\"):\n",
    "                counts['Empty'] += 1\n",
    "            elif not (is_number(lat) and is_number(lon)):\n",
    "                counts['Non_number'] += 1\n",
    "            elif not (is_within_bounds(float(lat), lat_bounds) and \n",
    "            is_within_bounds(float(lon), lon_bounds)):  \n",
    "            # If lat or lon is out of bounds:\n",
    "                counts['Out_of_bounds'] += 1\n",
    "            else:\n",
    "                counts['Correct'] += 1\n",
    "    return counts\n",
    "                \n",
    "pprint.pprint(check_positions(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that all the positional data appears to be correct. There is probably some screening done at the user input level when updating maps which stops errors from creeping in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.c. Postcode format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we are not checking if the postcodes make sense, merely if they have the correct format. French postcodes have 5 digits, the first two indicating the department (eg. 78 is for the Yvelines department, where most of our data comes from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Correct': 2365, 'Empty': 0, 'Incorrect': 0, 'Null': 0}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "postcode_re = re.compile(r'^\\d{5}$')  # Regex match to an isolated string of \n",
    "                                      # 5 consecutive digits\n",
    "\n",
    "def audit_postcodes(filename):\n",
    "    ''' Checks postocde to ensure every tag with an addr:postcode key has a postcode \n",
    "    value and that they follow the standard French postcode format (5 \n",
    "    consecutive digits)\n",
    "    In:\n",
    "        filename (string): Path to the file to check\n",
    "    Out:\n",
    "        dict: Values = Counts of the following occurences (used as keys):\n",
    "            Null -- there is a postcode key but no value (None)\n",
    "            Empty -- there is a postcode key but its value is \"\"\n",
    "            Incorrect -- there is a postcode key but its value does not follow \n",
    "                         the convention\n",
    "            Correct -- none of the above occurs\n",
    "    '''\n",
    "    counts = {'Null': 0, 'Empty': 0, 'Incorrect': 0, 'Correct': 0}\n",
    "    context = ET.iterparse(filename, events = ('start', )) # Detect starts only\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'node':\n",
    "            for elt in elem.findall('tag'):\n",
    "                if elt.get('k') == 'addr:postcode':  \n",
    "                # If the tag contains a postcode field\n",
    "                    pc = elt.get('v').strip()  # Then get the postcode value\n",
    "                    if pc == None:\n",
    "                        counts['Null'] += 1\n",
    "                    elif pc == \"\":\n",
    "                        counts['Empty'] += 1\n",
    "                    elif not re.match(postcode_re, pc): \n",
    "                    # If not in line with the 5-digit convention:\n",
    "                        counts['Incorrect'] += 1\n",
    "                    else:\n",
    "                        counts['Correct'] += 1\n",
    "    return counts\n",
    "\n",
    "pprint.pprint(audit_postcodes(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we seem to be in luck in that all the postcodes are correctly formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.d. Street / way types\n",
    "\n",
    "We will now focus on the different types of ways that exist in the document. Street names are human-edited and therefore prone to inconsistency and errors. The first step is therefore to list all the unique labels for streets and ways. To do this we adapt some code from the Intro to Data Wrangling course on Udacity.com.\n",
    "\n",
    "One of the adaptations is in the regular expression that is used to look for the street labels. In English, these would normally be positioned after the street name (\"Lexington Avenue\"), whereas in France, they are before (\"Avenue du Général Leclerc\").  \n",
    "\n",
    "Another point to keep in mind is that we need to convert strings to unicode because of special characters used in French but not in English, such as é, è, à, ô, ç etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Les',\n",
      " u'Ile',\n",
      " u'Centre',\n",
      " u'C.C.',\n",
      " u'Residence',\n",
      " u'Le',\n",
      " u\"Grand'Rue\",\n",
      " u'CCR',\n",
      " u'Otis',\n",
      " u'Allee',\n",
      " u'A\\xe9rodrome',\n",
      " u'Grande',\n",
      " u'\\xc9lys\\xe9e',\n",
      " u'Mail',\n",
      " u'Jean',\n",
      " u'Guyancourt']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'^\\S+\\.?', re.IGNORECASE)  # Regex to find \n",
    "# continuous groups of non-whitespace characters, each possibly ending with a .\n",
    "\n",
    "# List of expected types of streets and ways:\n",
    "expected = [u\"rue\", u\"avenue\", u\"boulevard\", \n",
    "            u\"route\", u\"place\", u\"villa\", \n",
    "            u\"impasse\", u\"passage\", u\"voie\", \n",
    "            u\"square\", u\"sentier\", u\"ruelle\",\n",
    "            u\"allée\", u\"chemin\", \n",
    "            u\"rond-point\", u\"cours\",\n",
    "            u\"parc\", u\"promenade\", u\"résidence\", \n",
    "            u\"domaine\", u\"quai\", u\"cour\", u\"clos\",\n",
    "            u\"autoroute\", u\"route nationale\", \n",
    "            u\"route départementale\", \n",
    "            u\"route communale\", u\"hameau\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    ''' For each street name, selects the first word (assumed to be street type) and\n",
    "    compares with the list of expected street types. \n",
    "    In case it is not there:\n",
    "    The street_types dict (passed by the upper environment) uses the unexpected \n",
    "    street type as a key and the full street name is added to the value (which\n",
    "    is a set).\n",
    "    \n",
    "    In: \n",
    "        street_types (dict of sets): Keys are unique unexpected street types, \n",
    "            values are sets of full street names with these street types. These \n",
    "            sets get updated in place by the function.\n",
    "        street_name (string): The street name to parse and compare with the list\n",
    "            of expected street types.\n",
    "    Out:\n",
    "        Nothing\n",
    "    '''\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = unicode(m.group())  # Convert to unicode because of French-specific characters\n",
    "        if street_type.lower() not in expected:\n",
    "            street_types[street_type].add(street_name)  # Record all streets \n",
    "            # with unexpected street types\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    ''' Checks whether a 'tag' element of the OSM XML file refers to a street name.\n",
    "    Returns True if this is the case, False if not.\n",
    "    '''\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "    \n",
    "\n",
    "def audit_all_streets(filename):\n",
    "    ''' Applies audit_street_type to all elements of the OSM XML file containing a\n",
    "    street name. Returns the collection of unexpected street types in the form \n",
    "    of a dictionary where key = unexpected street type, value = set of street\n",
    "    names with this street type.\n",
    "    \n",
    "    In:\n",
    "        filename (string): Path to the file to check\n",
    "    Out:\n",
    "        dict: Keys are unique street types not found in the reference list \n",
    "        (\"expected\"), values are sets containing all unique street names with\n",
    "        these street types.\n",
    "    '''\n",
    "    osm_file = open(filename, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    \n",
    "    for event, elem in ET.iterparse(osm_file, events = (\"start\",)): \n",
    "                                                            # Detect starts only\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):  # Iterate through all \"tag\" elements\n",
    "                if is_street_name(tag):   # Audit street type when the element \n",
    "                                          # contains a street name\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "street_types = audit_all_streets(filename)\n",
    "pprint.pprint(street_types.keys())  # Print unexpected street types (without the related street names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that aside from font case issues, the dataset is fairly clean. There are a few \"street\" labels that actually refer to shopping malls (or similar), an island (on the Seine river) and even to an airfield. The other issues are: missing accents on letters (Allee ilo. Allée), missing steet type (\"Jean Macé\" ilo. \"Rue Jean Macé\", \"Otis Mygatt\" ilo. \"Avenue Otis Mygatt\") or unusual street labels (\"Grand'Rue\" and \"Grande Rue\", where usually \"Rue\" would come first -- this is unusual but correct).  \n",
    "\n",
    "Some of these are acceptable but there are few genuine issues to fix:\n",
    "\n",
    "- 'Centre' (actually 'Centre Commercial Régional', but only the first word was picked up by the regular expression), C.C. and CCR all refer to the same shopping mall (the largest in the area) and should be updated to the full expression 'Centre Commercial Régional'.\n",
    "- 'Allee', 'Residence': Replace with 'Allée' and 'Résidence' respectively\n",
    "- 'Aérodrome' (airfield) is not a valid street type and should be stripped (investigating the document, we see that the node is already named 'Aeroclub' so the notion that it relates to airplanes is already present).\n",
    "- 'Elysée' is incomplete and refers to a \"Résidence\" (a housing program) named Elysée 2. We will therefore update it to \"Résidence Elysée 2\".\n",
    "- 'Otis' refers to 'Avenue Otis Mygatt'\n",
    "- 'Jean Macé' refers to 'Rue Jean Macé'\n",
    "- 'Guyancourt' is a town name. From the node IDs and their position, I was able infer that the tag refered to 'Rue Louis Breguet' in Guyancourt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centre commercial SQY Ouest => Centre Commercial SQY Ouest\n",
      "Centre commercial parly 2 => Centre Commercial parly 2\n",
      "C.C. Parly 2 => Centre Commercial Parly 2\n",
      "Residence de la Roseraie => Résidence de la Roseraie\n",
      "CCR Parly II => Centre Commercial Parly II\n",
      "Otis Mygatt => Avenue Otis Mygatt\n",
      "Allee Pierre de Coubertin => Allée Pierre de Coubertin\n",
      "Aérodrome - Rue du Docteur Vaillant => Rue du Docteur Vaillant\n",
      "Élysée 2 => Résidence Élysée 2\n",
      "Jean Macé => Rue Jean Macé\n",
      "Guyancourt => Rue Louis Breguet\n"
     ]
    }
   ],
   "source": [
    "'''Values to replace and to be repaced with:'''\n",
    "street_mapping = { u\"allee\": u\"Allée\",\n",
    "            u\"Allee\": u\"Allée\",\n",
    "            u\"hameau\": u\"Hameau\",\n",
    "            u\"Residence\": u\"Résidence\",\n",
    "            u\"résidence\": u\"Résidence\",\n",
    "            u\"Centre Commercial Régional\": u\"Centre Commercial\",\n",
    "            u\"Centre commercial\": u\"Centre Commercial\",\n",
    "            u\"C.C.\": u\"Centre Commercial\",\n",
    "            u\"CCR\": u\"Centre Commercial\",\n",
    "            u\"\\xc9lys\\xe9e 2\": u\"Résidence \\xc9lysée 2\",\n",
    "            u\"Aérodrome - \": u\"\",\n",
    "            u\"Otis\": u\"Avenue Otis\",\n",
    "            u\"Jean Macé\": u\"Rue Jean Macé\",\n",
    "            u\"Guyancourt\": u\"Rue Louis Breguet\"\n",
    "            }\n",
    "\n",
    "def update_street_name(street_name, mapping):\n",
    "    ''' Uses the mapping dictionary to correct street types: For each key in \n",
    "    mapping, looks for it in street_name and replaces this string with the \n",
    "    associated value (which is the correctly spelled street type).\n",
    "    \n",
    "    In: \n",
    "        street_name (string): Full street name possibly requiring a correction\n",
    "        mapping (dictionary): Keys are wrongly spelled street types, values are\n",
    "            strings to use as replacements within street_name.\n",
    "    Out:\n",
    "        string: Correctly spelled street name.\n",
    "    '''\n",
    "    for bad_name in mapping:\n",
    "        match = re.compile('^' + bad_name)\n",
    "        try:  # Substitute the string bad_name with the associated value in \n",
    "              # mapping. The rest of the street_name string is untouched.\n",
    "            street_name = re.sub(match, mapping[bad_name], street_name)\n",
    "        except LookupError:  # If this happens, we need to manually amend the \n",
    "                             # mapping.\n",
    "            print \"Missing entry in mapping\"\n",
    "            continue\n",
    "    return street_name\n",
    "\n",
    "for street_type, ways in street_types.iteritems():  # Do this for each unexpected street type\n",
    "    for street_name in ways:\n",
    "        better_name = update_street_name(street_name, street_mapping)\n",
    "        if street_name != better_name:  # Display replacements:\n",
    "            print street_name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that our cleaning function works correctly. We are going to use it on the fly when converting the data into a JSON file later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Accuracy and consistency\n",
    "In this section, we will assess whether the city and postcode data are complete and correct when compared to a data source that we know to be 100% accurate. The reference file we used is made available by La Poste (main French postal service) and can be downloaded [here](http://datanova.legroupe.laposte.fr/explore/dataset/laposte_hexasmal/download/?format=csv&timezone=Europe/Berlin&use_labels_for_header=true). The file is in CSV format with semi-column as separators as is the norm with French documents -- commas are used as decimal separators. It lists the 39,000 towns in France.\n",
    "\n",
    "We first have a look at all the (postcode, city) pairs that exist in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([(None, '78170'),\n",
      "     (None, 'Bougival'),\n",
      "     (None, 'Buc'),\n",
      "     (None, 'Croissy-sur-Seine'),\n",
      "     (None, 'Guyancourt'),\n",
      "     (None, 'La Celle-Saint-Cloud'),\n",
      "     (None, 'Le Chesnay'),\n",
      "     (None, u'Le V\\xe9sinet'),\n",
      "     (None, 'Marly-le-Roi'),\n",
      "     (None, 'Montesson'),\n",
      "     (None, 'Montigny-le-Bretonneux'),\n",
      "     (None, 'Noisy-le-Roi'),\n",
      "     (None, 'Roquencourt'),\n",
      "     (None, u\"Saint-Cyr-l'\\xc9cole\"),\n",
      "     (None, 'Saint-Germain-en-Laye'),\n",
      "     (None, 'Versailles'),\n",
      "     (None, 'Viroflay'),\n",
      "     (None, 'le Chesnay'),\n",
      "     ('78000', None),\n",
      "     ('78000', 'Versailles'),\n",
      "     ('78100', None),\n",
      "     ('78100', 'Saint-Germain-en-Laye'),\n",
      "     ('78101', None),\n",
      "     ('78103', None),\n",
      "     ('78110', None),\n",
      "     ('78110', 'Le Pecq'),\n",
      "     ('78110', u'Le V\\xe9sinet'),\n",
      "     ('78110', 'Saint-Germain-en-Laye'),\n",
      "     ('78112', None),\n",
      "     ('78112', 'Fourqueux'),\n",
      "     ('78120', 'Viroflay'),\n",
      "     ('78140', None),\n",
      "     ('78140', u'V\\xe9lizy Villacoublay'),\n",
      "     ('78140', u'V\\xe9lizy-Villacoublay'),\n",
      "     ('78150', None),\n",
      "     ('78150', 'Le Chesnay'),\n",
      "     ('78150', 'Rocquencourt'),\n",
      "     ('78160', None),\n",
      "     ('78160', 'Marly le Roi'),\n",
      "     ('78160', 'Marly-le-Roi'),\n",
      "     ('78170', None),\n",
      "     ('78170', 'LA CELLE SAINT-CLOUD'),\n",
      "     ('78170', 'La Celle-Saint-Cloud'),\n",
      "     ('78180', None),\n",
      "     ('78180', 'Montigny-le-Bretonneux'),\n",
      "     ('78190', None),\n",
      "     ('78210', None),\n",
      "     ('78210', \"Saint-Cyr l'Ecole\"),\n",
      "     ('78210', u\"Saint-Cyr-l'\\xc9cole\"),\n",
      "     ('78220', None),\n",
      "     ('78220', 'Viroflay'),\n",
      "     ('78220', 'viroflay'),\n",
      "     ('78230', None),\n",
      "     ('78230', 'Le Pecq'),\n",
      "     ('78240', None),\n",
      "     ('78240', 'Chambourcy'),\n",
      "     ('78280', None),\n",
      "     ('78280', 'Guyancourt'),\n",
      "     ('78280', 'Guyancourt '),\n",
      "     ('78290', None),\n",
      "     ('78290', 'Croissy sur Seine'),\n",
      "     ('78290', 'Croissy-sur-Seine'),\n",
      "     ('78330', None),\n",
      "     ('78330', 'Fontenay-le-Fleury'),\n",
      "     ('78350', None),\n",
      "     ('78350', 'Les Loges-en-Josas'),\n",
      "     ('78360', None),\n",
      "     ('78360', 'Montesson'),\n",
      "     ('78380', None),\n",
      "     ('78380', 'Bougival'),\n",
      "     ('78390', None),\n",
      "     ('78400', None),\n",
      "     ('78400', 'Chatou'),\n",
      "     ('78420', None),\n",
      "     ('78420', u'Carri\\xe8res-sur-Seine'),\n",
      "     ('78430', None),\n",
      "     ('78430', 'Louveciennes'),\n",
      "     ('78530', None),\n",
      "     ('78530', 'Buc'),\n",
      "     ('78530', 'Jouy-en-Josas'),\n",
      "     ('78560', None),\n",
      "     ('78560', 'Le Port-Marly'),\n",
      "     ('78590', None),\n",
      "     ('78590', 'Noisy le Roi'),\n",
      "     ('78590', 'Noisy le roi'),\n",
      "     ('78590', 'Noisy-le-Roi'),\n",
      "     ('78620', None),\n",
      "     ('78620', u\"L'\\xc9tang la Ville\"),\n",
      "     ('78620', u\"L'\\xc9tang-la-Ville\"),\n",
      "     ('78750', None),\n",
      "     ('78750', 'Mareil-Marly'),\n",
      "     ('78860', None),\n",
      "     ('78860', 'St Nom la breteche'),\n",
      "     ('78870', None),\n",
      "     ('78870', 'Bailly'),\n",
      "     ('78884', 'Saint Quentin en Yvelines'),\n",
      "     ('78960', None),\n",
      "     ('78960', 'Voisins-le-Bretonneux'),\n",
      "     ('92000', 'Nanterre'),\n",
      "     ('92410', None),\n",
      "     ('92420', None),\n",
      "     ('92420', 'Garches'),\n",
      "     ('92420', 'VAUCRESSON'),\n",
      "     ('92420', 'Vaucresson'),\n",
      "     ('92430', None),\n",
      "     ('92500', None),\n",
      "     ('92500', 'Rueil-Malmaison'),\n",
      "     ('92852', 'Rueil-Malmaison')])\n"
     ]
    }
   ],
   "source": [
    "pc_cities = set()\n",
    "    \n",
    "with open(filename, 'r') as osm_file:\n",
    "    for event, elem in ET.iterparse(osm_file, events = (\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == 'way':\n",
    "            pc_tag = elem.find(\"./tag[@k='addr:postcode']\")  # Use XPath command\n",
    "            # to locate postcode in node or way\n",
    "            try:\n",
    "                elem_pc = pc_tag.attrib['v']\n",
    "            except AttributeError:  # Case no tag attribute 'v'\n",
    "                elem_pc = None\n",
    "            \n",
    "            city_tag = elem.find(\"./tag[@k='addr:city']\")  # Use XPath command \n",
    "            # to locate city in node or way\n",
    "            try:\n",
    "                elem_city = city_tag.attrib['v']\n",
    "            except AttributeError:  # Case no tag attribute 'v'\n",
    "                elem_city = None\n",
    "            if not (elem_pc is None and elem_city is None): # As long as at \n",
    "            # least one of postcode or city name exists, add to set:\n",
    "                pc_cities.add((elem_pc, elem_city))\n",
    "\n",
    "pprint.pprint(pc_cities)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many consistency issues around spelling. Some cities have several different postcodes but this is expected (different post offices serve different areas of the same city). In many instances, we have a postcode but no city name. And in some cases, we have a city name but no postcode. We also have a case where a postcode was incorrectly entered as a city name ('78170').  \n",
    "To fix the majority of these issues, one idea would be to use the La Poste file to look postcodes up and update the city names with names from the file.  \n",
    "There is a handful of postcodes that even the reference file does not contain; this is because they are special postcodes used for priority distribution to companies (a paid service). We will correct these by way of a specific mapping dictionary once all the other issues are solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([('78000', 'Versailles'),\n",
      "     ('78100', 'St Germain En Laye'),\n",
      "     ('78101', 'St Germain En Laye'),\n",
      "     ('78103', 'St Germain En Laye'),\n",
      "     ('78110', 'Le Vesinet'),\n",
      "     ('78112', 'Fourqueux'),\n",
      "     ('78120', 'Sonchamp'),\n",
      "     ('78140', 'Velizy Villacoublay'),\n",
      "     ('78150', 'Le Chesnay'),\n",
      "     ('78150', 'Rocquencourt'),\n",
      "     ('78160', 'Marly Le Roi'),\n",
      "     ('78170', 'La Celle St Cloud'),\n",
      "     ('78180', 'Montigny Le Bretonneux'),\n",
      "     ('78190', 'Trappes'),\n",
      "     ('78210', 'St Cyr L Ecole'),\n",
      "     ('78220', 'Viroflay'),\n",
      "     ('78230', 'Le Pecq'),\n",
      "     ('78240', 'Chambourcy'),\n",
      "     ('78280', 'Guyancourt'),\n",
      "     ('78290', 'Croissy Sur Seine'),\n",
      "     ('78330', 'Fontenay Le Fleury'),\n",
      "     ('78350', 'Jouy En Josas'),\n",
      "     ('78350', 'Les Loges En Josas'),\n",
      "     ('78360', 'Montesson'),\n",
      "     ('78380', 'Bougival'),\n",
      "     ('78390', 'Bois D Arcy'),\n",
      "     ('78400', 'Chatou'),\n",
      "     ('78420', 'Carrieres Sur Seine'),\n",
      "     ('78430', 'Louveciennes'),\n",
      "     ('78530', 'Buc'),\n",
      "     ('78560', 'Le Port Marly'),\n",
      "     ('78590', 'Noisy Le Roi'),\n",
      "     ('78620', 'L Etang La Ville'),\n",
      "     ('78750', 'Mareil Marly'),\n",
      "     ('78860', 'St Nom La Breteche'),\n",
      "     ('78870', 'Bailly'),\n",
      "     ('78884', 'St Quentin En Yvelines'),\n",
      "     ('78960', 'Voisins Le Bretonneux'),\n",
      "     ('92000', 'Nanterre'),\n",
      "     ('92410', 'Ville D Avray'),\n",
      "     ('92420', 'Vaucresson'),\n",
      "     ('92430', 'Marnes La Coquette'),\n",
      "     ('92500', 'Rueil Malmaison'),\n",
      "     ('92852', 'Rueil Malmaison')])\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import csv\n",
    "\n",
    "city_to_pc_map = {\n",
    "     u'78170': '78170',\n",
    "     u'Bougival': '78380',\n",
    "     u'Buc': '78530',\n",
    "     u'Croissy-sur-Seine': '78290',\n",
    "     u'Guyancourt': '78280',\n",
    "     u'La Celle-Saint-Cloud': '78170',\n",
    "     u'Le Chesnay': '78150',\n",
    "     u'le Chesnay': '78150',\n",
    "     u'Le V\\xe9sinet': '78110',\n",
    "     u'Marly-le-Roi': '78160',\n",
    "     u'Montesson': '78360',\n",
    "     u'Montigny-le-Bretonneux': '78180',\n",
    "     u'Noisy-le-Roi': '78590',\n",
    "     u'Roquencourt': '78150',\n",
    "     u\"Saint-Cyr-l'\\xc9cole\": '78210',\n",
    "     u'Saint-Germain-en-Laye': '78100',\n",
    "     u'Versailles': '78000',\n",
    "     u'Viroflay': '78220'}\n",
    "\n",
    "pc_to_city_map = {\n",
    "    '78103': 'St Germain En Laye',\n",
    "    '78101': 'St Germain En Laye',\n",
    "    '78884': 'St Quentin En Yvelines',\n",
    "    '92852': 'Rueil Malmaison'\n",
    "}\n",
    "\n",
    "laposte_file = './Reference/laposte_hexasmal.csv'\n",
    "\n",
    "def parse_reference_file(filename):\n",
    "    ''' Take the reference file from La Poste and parse into a dictionaty with \n",
    "    postcode as keys. Several towns can have the same postcode, therefore each \n",
    "    dict value is a list.\n",
    "    \n",
    "    In: \n",
    "        filename (string): Path to the reference file to parse\n",
    "    Out:\n",
    "        dict of lists: Keys = postcodes, Values = lists of city names associated\n",
    "            to the key postcode.\n",
    "    '''    \n",
    "    data = defaultdict(list)\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter = ';') # French CSVs use ; separators\n",
    "        reader.next()\n",
    "        for row in reader:\n",
    "            data[str(row[2])].append(row[1].title())  # Use titlecase everywhere\n",
    "            # for consistency\n",
    "    return data\n",
    "    \n",
    "def get_pc_from_city(city):\n",
    "    ''' Returns postcode given city name, using the manually-defined \n",
    "    \"city_to_pc_map\" mapping'''\n",
    "    return city_to_pc_map[city]\n",
    "\n",
    "def get_city_from_pc(pc, city, reference):\n",
    "    ''' Return a correct city name given postcode and a possibly incorrect city \n",
    "    name, using the reference file. Because a postcode key can have several city\n",
    "    values in the reference file, we return the city name with the highest \n",
    "    similarity to the city name that we are trying to check/correct.\n",
    "    \n",
    "    In:\n",
    "        pc (string): Postcode for the PC / City combination we are assessing\n",
    "        city (string): City name for the same.\n",
    "        reference (dict of lists): Reference data parsed from the reference file\n",
    "            where keys = postcodes and values = lists of city names associated\n",
    "            with the key postcode.\n",
    "    Out:\n",
    "        string: Corrected city name.\n",
    "    '''\n",
    "    if city:\n",
    "        best_match = [None, 0.] \n",
    "        # Initialise: best_match[0] is for the best-matching city name, \n",
    "        # best_match[1] is for the match ratio of best_match[0] with the 'city' \n",
    "        # string.\n",
    "        for c in reference[pc]:  # For each city name associated with this \n",
    "                                 # particular postcode:\n",
    "            match_ratio = SequenceMatcher(None, c, city.title()).ratio()  \n",
    "            # Calculate similarity\n",
    "            if match_ratio >= best_match[1] or best_match[1] == 0:\n",
    "                best_match[0] = c  # This is our new best match\n",
    "                best_match[1] = match_ratio\n",
    "        #print 'City present:', city, 'Returning best match:', best_match[0]\n",
    "        return best_match[0]\n",
    "    else:  # If there is a postcode but no city name in the OSM data, we pick \n",
    "           # the first city name associated with this postcode in the reference \n",
    "           # dictionary.\n",
    "        try:          \n",
    "            reference[pc][0]  \n",
    "            # print 'No city: Returning first value:', reference[pc][0]\n",
    "            return reference[pc][0] \n",
    "        except IndexError:\n",
    "            # print 'No city found for postcode', pc\n",
    "            return\n",
    "        \n",
    "def correct_pc_city(pc, city, reference):\n",
    "    ''' Given a postcode and a city name (both potentially incorrect, and one or the\n",
    "    other can be missing -- note that if both are missing, this function is not \n",
    "    even called):\n",
    "    - Gets the correct postcode from the city name using get_pc_from_city() + \n",
    "    one \"manual\" adjustment to correct a typo in the data\n",
    "    - Once all cities have the correct postcode, fix all city names by applying\n",
    "    get_city_from_pc(). This ensures all city names are correct and follow the\n",
    "    same spelling and case conventions.\n",
    "    - Finally, correct four cases where special delivery postcodes are used, \n",
    "    which cannot be found in the reference data. This uses the pc_to_city_map\n",
    "    dictionary.\n",
    "\n",
    "    In:\n",
    "        pc (string): Postcode, potentially None or incorrect\n",
    "        city (string): City name, potentially None or incorrect\n",
    "        reference (dict of lists): Reference data parsed from the reference file\n",
    "            where keys = postcodes and values = lists of city names associated\n",
    "            with the key postcode.\n",
    "    Out:\n",
    "        tuple of strings: Corrected postcode, corrected city name\n",
    "    '''\n",
    "\n",
    "    '''First step: ensure each city has a valid postcode. '''\n",
    "    if pc is None: \n",
    "        new_pc = get_pc_from_city(city)  # Use the city name to get the postcode\n",
    "        #print 'No Postcode, returning value from map:', new_pc\n",
    "    else:\n",
    "        if pc == '78530' and city == 'Jouy-en-Josas':  # We found one typo where\n",
    "            # two postcodes got confused for one another\n",
    "            new_pc = '78350'\n",
    "        else:\n",
    "            new_pc = pc  # In the general case keep the same postcode.\n",
    "        #print 'Postcode present, returning same value (except Jouy en Josas):' \\\n",
    "        #, new_pc\n",
    "    \n",
    "    ''' Second step: ensure all postcodes have a correctly formatted city:'''\n",
    "    new_city = get_city_from_pc(new_pc, city, reference)\n",
    "\n",
    "    ''' Third step: correct the last four cases where postcode is \n",
    "    missing from the reference file, using our second manually-defined \n",
    "    mapping.'''\n",
    "    if new_pc in pc_to_city_map:\n",
    "        new_city = pc_to_city_map[new_pc]\n",
    "        \n",
    "    return new_pc, new_city\n",
    "\n",
    "            \n",
    "ref_data = parse_reference_file(laposte_file)\n",
    "\n",
    "pc_city = set()\n",
    "for pc, city in pc_cities:  # Apply correct_pc_city() to all postcode/city \n",
    "    # combinations found in the OSM XML file:\n",
    "    new_pc, new_city = correct_pc_city(pc, city, ref_data)\n",
    "    #print 'OUTCOME: ', new_pc, new_city, '\\n'\n",
    "    pc_city.add((new_pc, new_city))\n",
    "    \n",
    "pprint.pprint(pc_city)  # Outcome of our manipulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this process loses any accented characters as well as apostrophes and hyphens. This is because the La Poste file is meant to be a reference for handscript reading devices and therefore tends to simplify spelling as much as possible by using upper-cases (in French, accents are optional on upper-case characters). We believe it to be an acceptable trade-off for the improved quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Investigating the data with the MongoDB API\n",
    "### B.1. Data load into a MongoDB database\n",
    "\n",
    "Now that we developped functions to clean up the data, we need to shape it in a way that can be inserted into a MongoDB database. The general schema that we want to use is the same as in the Udacity case study, with one exception: We will hold latitude and longitude separately rather than in the same array, as this will come in handy later on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[...,\n",
    "    {\n",
    "    \"id\": \"2406124091\",\n",
    "    \"type: \"node\",\n",
    "    \"visible\":\"true\",\n",
    "    \"created\": {\n",
    "              \"version\":\"2\",\n",
    "              \"changeset\":\"17206049\",\n",
    "              \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "              \"user\":\"linuxUser16\",\n",
    "              \"uid\":\"1219059\"\n",
    "            },\n",
    "    \"lat\": 41.9757030,\n",
    "    \"lon\": -87.6921867,\n",
    "    \"address\": {\n",
    "              \"housenumber\": \"5157\",\n",
    "              \"postcode\": \"60625\",\n",
    "              \"street\": \"North Lincoln Ave\"\n",
    "            },\n",
    "    \"amenity\": \"restaurant\",\n",
    "    \"cuisine\": \"mexican\",\n",
    "    \"name\": \"La Cabana De Don Luis\",\n",
    "    \"phone\": \"1 (773)-271-5176\"\n",
    "    },\n",
    "...\n",
    "]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code used here has been written as part of the aforementioned case study, however we need to adapt it in order to include the various checks and corrections developped above. These functions are applied at each iteration just before writing it to the JSON file, as opposed to applying all corrections in one go to the entire data. This approach allows us to use iterative parsing on the OSM XML file. Although the data would easily fit into machine memory, this makes the code re-usable for larger pieces of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')  # Regex for two pieces\n",
    "# of lowercase strings separated by a colon character\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')  # Regex for any\n",
    "# potentially problematic character\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]  \n",
    "# Keys to retain in the 'created' field\n",
    "\n",
    "def process_tags(tag, tag_id):\n",
    "    ''' Parses node tags and way tags as per convention. Assumes no problematic \n",
    "    characters.\n",
    "    \n",
    "    In:\n",
    "        tag (XML element): A node or way's \"tag\" sub-element from the OSM XML\n",
    "            data\n",
    "        tag_id (string): The node or way's \"id\" attribute\n",
    "    Out:\n",
    "        dict: Contains the following key:value pairs:\n",
    "            'id': node or way id\n",
    "            'key': \"tag\" sub-element's key (anything after the colon in the \n",
    "                'k' attribute)\n",
    "            'value': \"tag\" sub-element's value ('v' attribute)\n",
    "            'type': \"tag\" sub-element's type (anything before the colon in the \n",
    "                'k' attribute) if there is a colon, None otherwise\n",
    "    '''\n",
    "    k = tag.get('k').strip()\n",
    "    v = tag.get('v').strip()\n",
    "    if re.match(lower_colon, k):  # If there is a ':' in the tag key:\n",
    "        tag_type, tag_key = k.split(':', 1)  # tag_type is anything before ':', \n",
    "                                             # tag_key is anything after\n",
    "        tag_value = v\n",
    "        if tag_type == \"addr\":\n",
    "            tag_type = \"address\"  # Use correct spelling\n",
    "    else:   # If there isn't a ':' in the tag key:\n",
    "        tag_type = None  # Returns None as the tag type\n",
    "        tag_key = k\n",
    "        tag_value = v\n",
    "    return  {'id': tag_id, 'key': tag_key, 'value': tag_value, 'type': tag_type}\n",
    "    # Return as a dict for convenience\n",
    "    \n",
    "def shape_element(element):\n",
    "    ''' Converts an XML element to the selected JSON schema.\n",
    "        \n",
    "    In:\n",
    "        element (XML element): An element from the OSM XML datafile.\n",
    "    Out:\n",
    "        dict: A dictionary with keys and values as per the defined JSON schema.\n",
    "    '''\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        ''' The following values are pulled directly from the XML data:'''\n",
    "        node['id'] = element.get('id')\n",
    "        node['type'] = element.tag\n",
    "        node['visible'] = element.get('visible')\n",
    "        node['created'] = {n: element.get(n) for n in CREATED} \n",
    "        # 'created' is a dict of dicts\n",
    "        try:\n",
    "            node['lat'] = float(element.get('lat'))  # Convert coordinates to \n",
    "                                                # floats if not already the case\n",
    "            node['lon'] = float(element.get('lon')) \n",
    "        except TypeError:\n",
    "            node['lat'] = element.get('lat')\n",
    "            node['lon'] = element.get('lon')\n",
    "        \n",
    "        '''\"Tag\" sub-elements require specific treatment:'''\n",
    "        for t in element.findall('tag'):  # For each sub-element tagged \"tag\":\n",
    "            if not re.match(problemchars, t.get('k')) and \\\n",
    "                len(re.findall(':', t.get('k'))) <= 1: \n",
    "            # (i.e. ignore when there are problem chars or more than one ':')\n",
    "                tag_content = process_tags(t, element.get('id'))  \n",
    "                # Parse tag and extract id, key, value and type\n",
    "                if tag_content['type'] is not None:  \n",
    "                # ie. if the 'k' field contained a colon character\n",
    "                    if (tag_content['type'] not in node.keys()) or \\\n",
    "                        not (isinstance(node[tag_content['type']], dict)):\n",
    "                    # Initialise an empty dict if there is no key of this 'type'\n",
    "                    # or its value is not already a dict\n",
    "                        node[tag_content['type']] = {}\n",
    "                    node[tag_content['type']][tag_content['key']] = \\\n",
    "                        tag_content['value']\n",
    "                    # for instance: node['address']['postcode'] = '78100'\n",
    "                else: # ie. if the 'k' field didn't contain a colon character\n",
    "                    node[tag_content['key']] = tag_content['value']\n",
    "        \n",
    "        ''' For \"nd\" tags, we collect all nodes contained in the way:'''\n",
    "        for t in element.findall('nd'):  \n",
    "            if 'node_refs' not in node.keys():\n",
    "                node['node_refs'] = []\n",
    "            node['node_refs'].append(t.get('ref'))\n",
    "                    \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    ''' Iterates through the OSM file and saves it into a correctly formatted JSON \n",
    "    file, applying cleaning procedures along the way.\n",
    "    \n",
    "    In:\n",
    "        file_in (string): Path to OSM XML file to clean up and convert to JSON\n",
    "        pretty (bool): If True, add whitespaces as required to the JSON file to\n",
    "            ensure a pretty formatting. False (default) for large data as \n",
    "            prettyfying is expansive.\n",
    "    Out:\n",
    "        list of dicts: JSON-formatted data, identical to the data saved to disk.\n",
    "    '''\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)  # Create a properly shaped JSON-type \n",
    "                                         # element as per defined schema.\n",
    "            if el:           \n",
    "                ''' Apply cleaning procedures to the address fields: \n",
    "                street names, postcodes and cities: '''\n",
    "                if 'address' in el: \n",
    "                    if 'street' in el['address']:  # Clean up street names\n",
    "                        el['address']['street'] = \\\n",
    "                            update_street_name(\n",
    "                                el['address']['street'], street_mapping\n",
    "                                              )\n",
    "                    if 'postcode' in el['address'] or 'city' in el['address']:  \n",
    "                    # Clean up postcodes & cities\n",
    "                    # We ensure either postcode or city (or both) is present\n",
    "                        try:\n",
    "                            pc = el['address']['postcode']  # Postcode present\n",
    "                        except KeyError:\n",
    "                            pc = None  # Postcode is absent\n",
    "                        try:\n",
    "                            city = el['address']['city']  # City present\n",
    "                        except KeyError:\n",
    "                            city = None  # City is absent\n",
    "                        el['address']['postcode'], el['address']['city'] = \\\n",
    "                            correct_pc_city(pc, city, ref_data) \n",
    "                        # Apply clean-up function to postcode and city name\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")  # Prefered for large \n",
    "                    # datasets as prettyfying the JSON file is expansive\n",
    "    return data\n",
    "\n",
    "data = process_map('./Versailles.osm/Versailles.osm', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a sample from the converted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created': {'changeset': '5055234',\n",
      "              'timestamp': '2010-06-22T21:58:33Z',\n",
      "              'uid': '11699',\n",
      "              'user': 'wagner51',\n",
      "              'version': '1'},\n",
      "  'id': '783430429',\n",
      "  'lat': 48.7628897,\n",
      "  'lon': 2.1218889,\n",
      "  'type': 'node',\n",
      "  'visible': None},\n",
      " {'created': {'changeset': '5375445',\n",
      "              'timestamp': '2010-08-01T21:38:37Z',\n",
      "              'uid': '158826',\n",
      "              'user': 'cquest',\n",
      "              'version': '1'},\n",
      "  'id': '842552161',\n",
      "  'lat': 48.8366723,\n",
      "  'lon': 2.1085662,\n",
      "  'type': 'node',\n",
      "  'visible': None},\n",
      " {'created': {'changeset': '8050159',\n",
      "              'timestamp': '2011-05-04T16:20:34Z',\n",
      "              'uid': '185687',\n",
      "              'user': 'apollinaire',\n",
      "              'version': '1'},\n",
      "  'id': '1272175173',\n",
      "  'lat': 48.871806,\n",
      "  'lon': 2.086614,\n",
      "  'type': 'node',\n",
      "  'visible': None},\n",
      " {'created': {'changeset': '5478160',\n",
      "              'timestamp': '2010-08-13T00:55:31Z',\n",
      "              'uid': '178790',\n",
      "              'user': 'ahmster',\n",
      "              'version': '1'},\n",
      "  'id': '857790569',\n",
      "  'lat': 48.8641687,\n",
      "  'lon': 2.0686098,\n",
      "  'type': 'node',\n",
      "  'visible': None},\n",
      " {'building': 'yes',\n",
      "  'created': {'changeset': '5603408',\n",
      "              'timestamp': '2010-08-26T23:59:41Z',\n",
      "              'uid': '11699',\n",
      "              'user': 'wagner51',\n",
      "              'version': '1'},\n",
      "  'id': '74630922',\n",
      "  'lat': None,\n",
      "  'lon': None,\n",
      "  'node_refs': ['881580001',\n",
      "                '881497086',\n",
      "                '881554148',\n",
      "                '881574514',\n",
      "                '881466265',\n",
      "                '881474359',\n",
      "                '881560242',\n",
      "                '881333646',\n",
      "                '881580001'],\n",
      "  'source': u'extraction vectorielle v1 cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadas. Mise \\xe0 jour : 2010',\n",
      "  'type': 'way',\n",
      "  'visible': None}]\n"
     ]
    }
   ],
   "source": [
    "from random import sample, seed\n",
    "\n",
    "seed(123)\n",
    "pprint.pprint(sample(data, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems correctly converted to JSON. We now need to load it into a MongoDB database. This is achieved with the `mongoimport` command from the OS command line. The data takes about 30 seconds to load and we get the following prompt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "2016-12-16T09:47:56.233+0200    connected to: localhost\n",
    "2016-12-16T09:47:59.226+0200    [########################] OpenStreetMap.Versailles     281MB/31.2MB (899.0%)\n",
    "2016-12-16T09:48:02.225+0200    [########################] OpenStreetMap.Versailles     281MB/62.9MB (446.3%)\n",
    "2016-12-16T09:48:05.226+0200    [########################] OpenStreetMap.Versailles     281MB/95.0MB (295.4%)\n",
    "2016-12-16T09:48:08.226+0200    [########################] OpenStreetMap.Versailles     281MB/127MB (220.4%)\n",
    "2016-12-16T09:48:11.226+0200    [########################] OpenStreetMap.Versailles     281MB/159MB (176.2%)\n",
    "2016-12-16T09:48:14.225+0200    [########################] OpenStreetMap.Versailles     281MB/192MB (146.0%)\n",
    "2016-12-16T09:48:17.226+0200    [########################] OpenStreetMap.Versailles     281MB/230MB (122.1%)\n",
    "2016-12-16T09:48:20.225+0200    [########################] OpenStreetMap.Versailles     281MB/276MB (101.8%)\n",
    "2016-12-16T09:48:20.575+0200    [########################] OpenStreetMap.Versailles     281MB/281MB (100.0%)\n",
    "2016-12-16T09:48:20.576+0200    imported 1167432 documents\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2. Data investigations with `pymongo`\n",
    "We will use `pymongo` to query the database from Python. We therefore connect to the database using `MongoClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    ''' Creates connection to local MongoDB database.\n",
    "    '''\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client.OpenStreetMap\n",
    "    return db\n",
    "\n",
    "db = get_db()\n",
    "\n",
    "def aggregate(query):\n",
    "    ''' Runs a MongoDB aggregation query on the connected database.\n",
    "\n",
    "    In:\n",
    "        query (string): An query using MongoDB's aggregation framework\n",
    "    Out:\n",
    "        list: List of documents returned by the query\n",
    "    '''    \n",
    "    return [doc for doc in db.Versailles.aggregate(query)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2.a. General statistics\n",
    "* **File size:**  \n",
    "From the screen capture above, we see that the uncompressed data is 281MB once converted to JSON. Thus, the conversion from OSM XML caused a 25% increase in file size.  \n",
    "\n",
    "\n",
    "* **Number of documents:**  \n",
    "Again, we can get this information from the screen capture above, but also programmatically with the following query:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "doc_count = [{\"$group\": {\"_id\": \"Number of documents\", \n",
    "                         \"count\": {\"$sum\": 1}}}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Number of documents', u'count': 1167432}]\n"
     ]
    }
   ],
   "source": [
    "doc_count = [{\"$group\": {\"_id\": \"Number of documents\", \"count\":{\"$sum\": 1}}}]\n",
    "pprint.pprint(aggregate(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have just used a combination of .find() and .count() to get to the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of nodes:**  \n",
    "We use the following query:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "node_count = [{\"$match\": {\"type\": \"node\"}},\n",
    "              {$group\": {\"_id\": \"type\", \"count\": {\"$sum\": 1}}}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'node', u'count': 1007491}]\n"
     ]
    }
   ],
   "source": [
    "node_count = [{\"$match\": {\"type\": \"node\"}},\n",
    "              {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}}]\n",
    "pprint.pprint(aggregate(node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of ways:**  \n",
    "We use:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "way_count = [{\"$match\": {\"type\": \"way\"}},\n",
    "             {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'way', u'count': 159877}]\n"
     ]
    }
   ],
   "source": [
    "way_count = [{\"$match\": {\"type\": \"way\"}},\n",
    "             {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}}]\n",
    "pprint.pprint(aggregate(way_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2.b. Specific statistics\n",
    "\n",
    "* **Number of unique users who edited the data:**  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "unique_uids = [{\"$group\": {\"_id\": \"$created.uid\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$group\": {\"_id\": \"Unique users\", \"count\": {\"$sum\": 1}}}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Unique users', u'count': 1014}]\n"
     ]
    }
   ],
   "source": [
    "unique_uids = [{\"$group\": {\"_id\": \"$created.uid\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$group\": {\"_id\": \"Unique users\", \"count\": {\"$sum\": 1}}}]\n",
    "pprint.pprint(aggregate(unique_uids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Top 10 contributors:**  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "top10_users = [{\"$group\": {\"_id\": \"$created.uid\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$sort\": {\"count\": -1}},\n",
    "               {\"$limit\": 10}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'wagner51', u'count': 324461},\n",
      " {u'_id': u'Marcussacapuces91', u'count': 168536},\n",
      " {u'_id': u'osmmaker', u'count': 136707},\n",
      " {u'_id': u'didier2020', u'count': 101247},\n",
      " {u'_id': u'Dave92', u'count': 44346},\n",
      " {u'_id': u'cquest', u'count': 36809},\n",
      " {u'_id': u'PierenBot', u'count': 27341},\n",
      " {u'_id': u'apollinaire', u'count': 21435},\n",
      " {u'_id': u'uploaddidier2020', u'count': 19151},\n",
      " {u'_id': u'alexis78', u'count': 18936}]\n"
     ]
    }
   ],
   "source": [
    "top10_users = [{\"$group\": {\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$sort\": {\"count\": -1}},\n",
    "               {\"$limit\": 10}]\n",
    "pprint.pprint(aggregate(top10_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of nodes per city (top 10):**  \n",
    "We only retain \"node\" elements that contain a city name:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "top10_cities = [{\"$match\": {\"address.city\": {\"$exits\": 1}}},\n",
    "                {\"$group\": {\"_id\": \"$address.city\", \"count\": {\"$sum\": 1}}},\n",
    "                {\"$sort\": {\"count\": -1}},\n",
    "                {\"$limit\": 10}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'St Germain En Laye', u'count': 824},\n",
      " {u'_id': u'Versailles', u'count': 754},\n",
      " {u'_id': u'Buc', u'count': 487},\n",
      " {u'_id': u'Le Vesinet', u'count': 293},\n",
      " {u'_id': u'St Cyr L Ecole', u'count': 272},\n",
      " {u'_id': u'Marly Le Roi', u'count': 270},\n",
      " {u'_id': u'Viroflay', u'count': 135},\n",
      " {u'_id': u'Chatou', u'count': 123},\n",
      " {u'_id': u'Noisy Le Roi', u'count': 121},\n",
      " {u'_id': u'Le Pecq', u'count': 83}]\n"
     ]
    }
   ],
   "source": [
    "top10_cities = [{\"$match\": {\"address.city\": {\"$exists\": 1}}},\n",
    "                {\"$group\": {\"_id\": \"$address.city\", \"count\": {\"$sum\": 1}}},\n",
    "                {\"$sort\": {\"count\": -1}},\n",
    "                {\"$limit\": 10}]\n",
    "pprint.pprint(aggregate(top10_cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ranking is surprising because we would instinctively expect the number of node to be somewhat correlated to the population of each town. Here is the population for these towns, in the same order ([source](http://www.toutes-les-villes.com/villes-departements/78-yvelines/1.html)):\n",
    "\n",
    "|City / Town:         |Population|\n",
    "|---------------------|----------|\n",
    "|St Germain en Laye   |    38,124|\n",
    "|Versailles           |    85,761|\n",
    "|Buc                  |     5,743|\n",
    "|Le Vésinet           |    15,928|\n",
    "|St Cyr L'Ecole       |    14,585|\n",
    "|Marly Le Roi         |    16,787|\n",
    "|Viroflay             |    15,205|\n",
    "|Chatou               |    28,582|\n",
    "|Noisy Le Roi         |     7,711|\n",
    "|Le Pecq              |    16,342|\n",
    "\n",
    "Of course, the boundaries of the map extract are rectangular and therefore some of the cities will be incomplete. However it is still strange to see that Versailles, the largest city by far (and the most touristic), is only second in the ranking while Buc is number 3 while being the smallest of these 10.\n",
    "One possible explanation is that some of the most active contributors have focused on a few areas only (typically, where they live).  \n",
    "\n",
    "Let us test this theory:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# First we need to retrieve the top 10 users, but only for elements that have a city name:\n",
    "match = [{\"$match\": {\"address.city\": {\"$exists\": 1}}},\n",
    "         {\"$group\": {\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$sort\": {\"count\": -1}},\n",
    "               {\"$limit\": 10},\n",
    "         {\"$project\": {\"_id\": 1}}]\n",
    "         \n",
    "# This gives us a 'user_list' (see code for details)\n",
    "\n",
    "# Use this list to filter down users:\n",
    "cities_by_user = [{\"$match\": {\"created.user\": {\"$in\": user_list},\n",
    "                              \"address.city\": {\"$ne\": None}}},\n",
    "                  {\"$group\": \n",
    "                       {\"_id\": {\"user\": \"$created.user\", \"city\": \"$address.city\"},\n",
    "                        \"count\": {\"$sum\": 1}}},\n",
    "                  {\"$sort\": {\"_id.user\": 1}}]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 contributors for elements that contain a city name:\n",
      "[u'feuerbach78', u'osmmaker', u'wagner51', u'Guillaume Castagnino', u'didier2020', u'GeorgeKaplan', u'XavierT', u'Marcussacapuces91', u'Carte-on', u'denis1378']\n",
      "\n",
      "Number of contribution by these 10 users, by city:\n",
      "[{u'_id': {u'city': u'Buc', u'user': u'Carte-on'}, u'count': 61},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'Carte-on'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Montigny Le Bretonneux', u'user': u'Carte-on'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'Guyancourt', u'user': u'GeorgeKaplan'}, u'count': 1},\n",
      " {u'_id': {u'city': u'St Cyr L Ecole', u'user': u'GeorgeKaplan'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Le Chesnay', u'user': u'GeorgeKaplan'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Le Vesinet', u'user': u'GeorgeKaplan'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Croissy Sur Seine', u'user': u'GeorgeKaplan'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'St Germain En Laye', u'user': u'GeorgeKaplan'},\n",
      "  u'count': 27},\n",
      " {u'_id': {u'city': u'Chatou', u'user': u'GeorgeKaplan'}, u'count': 2},\n",
      " {u'_id': {u'city': u'Noisy Le Roi', u'user': u'GeorgeKaplan'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Le Port Marly', u'user': u'GeorgeKaplan'}, u'count': 28},\n",
      " {u'_id': {u'city': u'Rueil Malmaison', u'user': u'GeorgeKaplan'},\n",
      "  u'count': 4},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'GeorgeKaplan'}, u'count': 17},\n",
      " {u'_id': {u'city': u'Fontenay Le Fleury', u'user': u'GeorgeKaplan'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'city': u'St Cyr L Ecole', u'user': u'Guillaume Castagnino'},\n",
      "  u'count': 255},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'Marcussacapuces91'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'Montesson', u'user': u'Marcussacapuces91'}, u'count': 4},\n",
      " {u'_id': {u'city': u'Marly Le Roi', u'user': u'Marcussacapuces91'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'Bougival', u'user': u'Marcussacapuces91'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Le Vesinet', u'user': u'Marcussacapuces91'},\n",
      "  u'count': 58},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'XavierT'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Viroflay', u'user': u'XavierT'}, u'count': 70},\n",
      " {u'_id': {u'city': u'Chatou', u'user': u'denis1378'}, u'count': 59},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'didier2020'}, u'count': 2},\n",
      " {u'_id': {u'city': u'Le Pecq', u'user': u'didier2020'}, u'count': 1},\n",
      " {u'_id': {u'city': u'St Germain En Laye', u'user': u'didier2020'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'Bailly', u'user': u'didier2020'}, u'count': 42},\n",
      " {u'_id': {u'city': u'Noisy Le Roi', u'user': u'didier2020'}, u'count': 106},\n",
      " {u'_id': {u'city': u'Velizy Villacoublay', u'user': u'didier2020'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'city': u'Mareil Marly', u'user': u'feuerbach78'}, u'count': 2},\n",
      " {u'_id': {u'city': u'Croissy Sur Seine', u'user': u'feuerbach78'},\n",
      "  u'count': 27},\n",
      " {u'_id': {u'city': u'Le Vesinet', u'user': u'feuerbach78'}, u'count': 197},\n",
      " {u'_id': {u'city': u'St Germain En Laye', u'user': u'feuerbach78'},\n",
      "  u'count': 625},\n",
      " {u'_id': {u'city': u'Marly Le Roi', u'user': u'feuerbach78'}, u'count': 241},\n",
      " {u'_id': {u'city': u'Chatou', u'user': u'feuerbach78'}, u'count': 3},\n",
      " {u'_id': {u'city': u'Le Pecq', u'user': u'feuerbach78'}, u'count': 55},\n",
      " {u'_id': {u'city': u'Bougival', u'user': u'osmmaker'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Le Vesinet', u'user': u'osmmaker'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Nanterre', u'user': u'osmmaker'}, u'count': 27},\n",
      " {u'_id': {u'city': u'Viroflay', u'user': u'osmmaker'}, u'count': 1},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'osmmaker'}, u'count': 466},\n",
      " {u'_id': {u'city': u'Rueil Malmaison', u'user': u'osmmaker'}, u'count': 1},\n",
      " {u'_id': {u'city': u'St Germain En Laye', u'user': u'osmmaker'}, u'count': 8},\n",
      " {u'_id': {u'city': u'Montigny Le Bretonneux', u'user': u'wagner51'},\n",
      "  u'count': 21},\n",
      " {u'_id': {u'city': u'Versailles', u'user': u'wagner51'}, u'count': 2},\n",
      " {u'_id': {u'city': u'Jouy En Josas', u'user': u'wagner51'}, u'count': 9},\n",
      " {u'_id': {u'city': u'Buc', u'user': u'wagner51'}, u'count': 389}]\n"
     ]
    }
   ],
   "source": [
    "'''Cities edited by top 10 contributors:'''\n",
    "\n",
    "# First we need to retrieve the top 10 users, but only for elements that have a city name:\n",
    "match = [{\"$match\": {\"address.city\": {\"$exists\": 1}}},\n",
    "         {\"$group\": {\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "               {\"$sort\": {\"count\": -1}},\n",
    "               {\"$limit\": 10},\n",
    "         {\"$project\": {\"_id\": 1}}]\n",
    "\n",
    "# Turn the result into a list of users:\n",
    "user_list = [u[u'_id'] for u in aggregate(match)]\n",
    "print \"Top 10 contributors for elements that contain a city name:\"\n",
    "print user_list\n",
    "\n",
    "# Use this list to filter down users:\n",
    "cities_by_user = [{\"$match\": {\"created.user\": {\"$in\": user_list},\n",
    "                              \"address.city\": {\"$ne\": None}}},\n",
    "                  {\"$group\": \n",
    "                       {\"_id\": {\"user\": \"$created.user\", \"city\": \"$address.city\"},\n",
    "                        \"count\": {\"$sum\": 1}}},\n",
    "                  {\"$sort\": {\"_id.user\": 1}}]\n",
    "print \"\\nNumber of contribution by these 10 users, by city:\"\n",
    "pprint.pprint(aggregate(cities_by_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is the number of elements by city, from the top 10 contributors on elements containing city names. We notice several features:\n",
    "- User \"wagner51\" contributed in only 3 cities, with 389 edits to Buc (out of a total of 487 for this small town)\n",
    "- User \"feuerbach78\" contributed in 7 cities, including St-Germain-en-Laye for 625 edits (out of a total of 824 for this city) and Marly-Le-Roi for 241 edits (out of 270)\n",
    "- User \"didier2020\" contributed in 6 cities, most of their edits were against Noisy-Le-Roi where he accounts for 106 out of 121 edits.\n",
    "\n",
    "It appears that OpenStreetMap being a relatively unknown application (at least in France), there is a very unbalanced distribution in the number of edits per user, with a few users generating the vast majority of the elements. This makes statistical comparisons with the number of inhabitants of a particular city are largely irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2.c. Visiting the Château de Versailles\n",
    "\n",
    "There is a lot of information that we may want to get from the database about the Versailles palace. We will give a few examples below.  \n",
    "The first thing to define is what is considered as the palace and its grounds. Looking at the map, we can roughly approximate it with a horizontal, rectangular box between latitudes [48.801217, 48.820209] and longitudes [2.079505, 2.123966].  \n",
    "\n",
    "So as a tourist, how many points of interest can you expect to see?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "tourism_spots = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": {\"$exists\": 1}\n",
    "                      }\n",
    "                 },\n",
    "                 {\"$group\": {\"_id\": \"$tourism\", \"count\": {\"$sum\": 1}}}\n",
    "                ]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'caravan_site', u'count': 2},\n",
      " {u'_id': u'picnic_site', u'count': 2},\n",
      " {u'_id': u'viewpoint', u'count': 2},\n",
      " {u'_id': u'yes', u'count': 1},\n",
      " {u'_id': u'information', u'count': 16},\n",
      " {u'_id': u'museum', u'count': 4},\n",
      " {u'_id': u'hotel', u'count': 7},\n",
      " {u'_id': u'attraction', u'count': 14},\n",
      " {u'_id': u'artwork', u'count': 204}]\n"
     ]
    }
   ],
   "source": [
    "tourism_spots = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": {\"$exists\": 1}\n",
    "                      }\n",
    "                 },\n",
    "                 {\"$group\": {\"_id\": \"$tourism\", \"count\": {\"$sum\": 1}}}\n",
    "                ]\n",
    "\n",
    "pprint.pprint(aggregate(tourism_spots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some these categories are somewhat unspecific. Let us look into \"attractions\" in more detail:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "attractions = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": \"attraction\"\n",
    "                      }\n",
    "                 },\n",
    "                 {\"$project\": {\"_id\": 0,\n",
    "                               \"attraction_name\": \"$name\"}}\n",
    "                ]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'attraction_name': u'Le hameau de la reine'},\n",
      " {u'attraction_name': u'Ferme'},\n",
      " {u'attraction_name': u'P\\xe9ristyle'},\n",
      " {u'attraction_name': u'Trianon-sous-Bois'},\n",
      " {u'attraction_name': u'Orangerie'},\n",
      " {u'attraction_name': u'Aile Nord'},\n",
      " {u'attraction_name': u'Aile Sud'},\n",
      " {u'attraction_name': u'Galerie des Cotelle'},\n",
      " {u'attraction_name': u'Grand Appartement de la Reine'},\n",
      " {u'attraction_name': u'Grand Appartement du Roi'},\n",
      " {u'attraction_name': u'Grotte'},\n",
      " {u'attraction_name': u'Salles des Croisades'},\n",
      " {u'attraction_name': u'solitude'},\n",
      " {u'attraction_name': u'Le ch\\xeane de Louis XIV'}]\n"
     ]
    }
   ],
   "source": [
    "attractions = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": \"attraction\"\n",
    "                      }\n",
    "                 },\n",
    "                 {\"$project\": {\"_id\": 0,\n",
    "                               \"attraction_name\": \"$name\"}}\n",
    "                ]\n",
    "\n",
    "pprint.pprint(aggregate(attractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can confirm that these are all well worth your time -- Some of them are part of the main building, while others are separate buidings or features. The \"Hameau de la Reine\" for instance is a charming little hamlet that was built to satisfy Queen Marie-Antoinette's fantasies of living a rural life.\n",
    "\n",
    "![hameau_de_la_reine](img/PA0111-hr.jpg)\n",
    "\n",
    "The Orangerie is also very nice. It is still used to grow orange trees and other species. They can be carried inside the building in the winter or taken outside in the spring and summer:\n",
    "\n",
    "![orangerie](img/Garden-orangerie_Exterior_of_the_Palace_of_Versailles.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about these 204 artworks reported in the first query about the Versailles palace?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "artworks = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": \"artwork\"\n",
    "                      }\n",
    "             },\n",
    "             {\"$group\": {\"_id\": \"$artwork_type\",\n",
    "                         \"count\": {\"$sum\": 1}}}\n",
    "           ]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'statue', u'count': 1},\n",
      " {u'_id': u'sculpture', u'count': 183},\n",
      " {u'_id': None, u'count': 20}]\n"
     ]
    }
   ],
   "source": [
    "artworks = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"tourism\": \"artwork\"\n",
    "                      }\n",
    "             },\n",
    "             {\"$group\": {\"_id\": \"$artwork_type\",\n",
    "                         \"count\": {\"$sum\": 1}}}\n",
    "           ]\n",
    "\n",
    "pprint.pprint(aggregate(artworks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "184 sculptures and statues! This is actually far from the full collection, as there are over 300 statues in the castle grounds alone. The data is therefore about 60% complete as far as sculptures go.\n",
    "\n",
    "How about fountains?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "fountains = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"amenity\": \"fountain\"\n",
    "                      }\n",
    "              },\n",
    "              {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\": 1}}}\n",
    "            ]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'fountain', u'count': 35}]\n"
     ]
    }
   ],
   "source": [
    "fountains = [{\"$match\": \n",
    "                      {\"lat\": {\"$gte\": 48.801217},\n",
    "                       \"lat\": {\"$lte\": 48.828209},\n",
    "                       \"lon\": {\"$gte\": 2.079505},\n",
    "                       \"lon\": {\"$lte\": 2.123966},\n",
    "                       \"amenity\": \"fountain\"\n",
    "                      }\n",
    "              },\n",
    "              {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\": 1}}}\n",
    "            ]\n",
    "\n",
    "pprint.pprint(aggregate(fountains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://fr.wikipedia.org/wiki/Jardin_de_Versailles) we learn that there are actually 55 fountains on the Versailles castle grounds. So again, the data is about 64% complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Ideas for further improvement / exploration\n",
    "\n",
    "### C.1. Use reference data for street names\n",
    "To further clean up the data, one could compare street names with a record of all existing street names in the area. Such databases are available but quite expensive (see [here](https://gumroad.com/l/VzKNO) for instance). The cleaning up process would then need to compare each street in the OSM file with this reference, city by city, and compute similarities to pick the most likely street name (like we did for city names earlier in this project).\n",
    "\n",
    "* **Benefits:**\n",
    "    - Data is probably exhaustive and accurate\n",
    "    - Data already in an easily usable format\n",
    "\n",
    "* **Drawbacks / potential problems:**\n",
    "    - Cost\n",
    "    - Data would need to be purchased and processed on a regular basis to remain up-to-date\n",
    "\n",
    "### C.2. Use data from the Google Maps API\n",
    "Google Map has very comprehensive data and it is made available through an API. We could use this to enrich our dataset, especially with regards to street names again.\n",
    "\n",
    "If we wanted to stay consistent our \"tourist\" approach, we could also pull out a list of all hotels, restaurants and public transportation in the area by using search functions, then include them into our dataset.\n",
    "\n",
    "* **Benefits:**\n",
    "    - Reference data is generally high quality and updated regularly\n",
    "\n",
    "* **Drawbacks / potential problems:**\n",
    "    - Usage rights?\n",
    "    - API learning curve?\n",
    "\n",
    "### C.3. Organise \"field days\"\n",
    "We saw that the vast majority of the data is supplied by only a few contributors who tend to focus on small geographical areas. The two obvious ways to increase the number and span of contributions are:\n",
    "\n",
    "- Get more people to contribute\n",
    "- Get the heavy contributors to submit data outside of their usual area\n",
    "\n",
    "To do this, one idea could be to organise field days where OpenStreetMap users could go out in a given area in competing teams. Using a dedicated smartphone app, they would take pictures of street plaques and buildings. These pictures would be then processed by a machine learning system and/or manually commented and, using geo-tagging, would help complete the OSM data.\n",
    "\n",
    "* **Benefits:**\n",
    "    - Increase involvement of users and create a real community\n",
    "    - Allow targetting of the areas to improve\n",
    "    \n",
    "* **Drawbacks / potential problems:**\n",
    "    - Requires a coordination team to organise and manage\n",
    "    - Investment in smartphone app and rewards\n",
    "\n",
    "I hope you enjoyed this exploration of the Versailles area and its palace and that you learned some interesting facts beside the technical aspects of the project. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
